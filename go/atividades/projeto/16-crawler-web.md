# Projeto 16: Web Crawler Simples

## ğŸ“ DescriÃ§Ã£o
Crie um crawler que navega por pÃ¡ginas web, extrai links e informaÃ§Ãµes usando net/http e HTML parsing.

## ğŸ¯ Requisitos

### Funcionalidades ObrigatÃ³rias
1. **Crawling**:
   - Iniciar de uma URL
   - Baixar pÃ¡gina HTML
   - Extrair links
   - Seguir links (com limite de profundidade)
   - Evitar loops (nÃ£o visitar mesma URL duas vezes)

2. **ExtraÃ§Ã£o**:
   - Extrair tÃ­tulos
   - Extrair textos
   - Extrair imagens
   - Extrair metadados

3. **Controle**:
   - Limite de pÃ¡ginas
   - Limite de profundidade
   - Filtro de domÃ­nios

4. **ConcorrÃªncia**: Processar mÃºltiplas URLs em paralelo (worker pool)

5. **PersistÃªncia**: Salvar resultados em JSON

## ğŸ“š Conceitos Utilizados
- âœ… net/http
- âœ… HTML parsing (goquery ou regex)
- âœ… Goroutines
- âœ… Channels
- âœ… Worker pool
- âœ… Slices e maps
- âœ… Error handling
- âœ… ConcorrÃªncia avanÃ§ada

## ğŸ“ Estrutura Sugerida
```
crawler/
â”œâ”€â”€ main.go
â”œâ”€â”€ crawler.go
â”œâ”€â”€ parser.go
â”œâ”€â”€ worker.go
â””â”€â”€ README.md
```

## ğŸ’¡ ImplementaÃ§Ã£o Sugerida

### Structs
```go
type Pagina struct {
    URL     string
    Titulo  string
    Links   []string
    Texto   string
    Profundidade int
}

type Crawler struct {
    visitadas map[string]bool
    limite    int
    workers   int
}
```

### ConcorrÃªncia
- Worker pool para processar URLs
- Channel para fila de URLs
- Mutex para map de visitadas

## âœ… CritÃ©rios de Sucesso
- [ ] Crawling funciona
- [ ] ExtraÃ§Ã£o Ã© precisa
- [ ] ConcorrÃªncia funciona
- [ ] Loops sÃ£o evitados
- [ ] Limites sÃ£o respeitados
- [ ] Dados sÃ£o salvos

## ğŸš€ Extras (Desafio)
- [ ] Robots.txt respect
- [ ] Sitemap generation
- [ ] Exportar para diferentes formatos
- [ ] AnÃ¡lise de conteÃºdo
- [ ] VisualizaÃ§Ã£o de grafo de links

